{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modulo 2 - 8. Dense + Dropout + Batch Normalization\n",
    "\n",
    "Por Juan Pablo Bernal Lafarga - A01742342"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Student GPA dataset to predict student GPA.\n",
    "\n",
    "Use previous concepts to create different Neural Network Architectures and compare your results. (Python Notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El propósito del siguiente bloque es cargar y preprocesar un conjunto de datos de rendimiento estudiantil seleccionando variables relevantes dividiendo en conjuntos de entrenamiento y prueba estandarizando las variables predictoras y verificando el tamaño del conjunto de datos de entrenamiento para preparar el entrenamiento de una red neuronal que prediga el GPA de los estudiantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1913, 11)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = pd.read_csv(\"Student_performance_data _.csv\")\n",
    "dataset = data[['Age','ParentalEducation','StudyTimeWeekly','Absences','Tutoring','ParentalSupport','Extracurricular','Sports','Music','Volunteering','GPA','GradeClass']]\n",
    "\n",
    "X = dataset.drop('GPA', axis = 1)\n",
    "y = dataset['GPA']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.fit_transform(X_test)\n",
    "\n",
    "np.shape(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: A single Dense Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim = 11, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mean_absolute_error'])\n",
    "\n",
    "history = model.fit(X_train_std, y_train, epochs = 25, batch_size=5, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: A set of three Dense Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(64, input_dim = 11, activation='relu'))\n",
    "model2.add(Dense(32))\n",
    "model2.add(Dense(16))\n",
    "model2.add(Dense(1))\n",
    "\n",
    "model2.compile(optimizer='adam', loss='mse', metrics=['mean_absolute_error'])\n",
    "\n",
    "history2 = model2.fit(X_train_std, y_train, epochs = 25, batch_size=5, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Add a dropout layer after each Dense Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Dense(64, input_dim = 11, activation='relu'))\n",
    "model3.add(Dropout(0.15))\n",
    "model3.add(Dense(32))\n",
    "model3.add(Dropout(0.15))\n",
    "model3.add(Dense(16))\n",
    "model3.add(Dropout(0.15))\n",
    "model3.add(Dense(1))\n",
    "\n",
    "model3.compile(optimizer='adam', loss='mse', metrics=['mean_absolute_error'])\n",
    "\n",
    "history3 = model3.fit(X_train_std, y_train, epochs = 25, batch_size=5, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: Add a Batch Normalization Layer after each Dropout Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Dense(64, input_dim = 11, activation='relu'))\n",
    "model4.add(Dropout(0.2)) # Tira el 20% \n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Dense(32))\n",
    "model4.add(Dropout(0.2))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Dense(16))\n",
    "model4.add(Dropout(0.2))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Dense(1))\n",
    "\n",
    "model4.compile(optimizer='adam', loss='mse', metrics=['mean_absolute_error'])\n",
    "\n",
    "history4 = model4.fit(X_train_std, y_train, epochs = 25, batch_size=5, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a comparative table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo 1. Loss: 0.029858337715268135, MAE: 0.13903826475143433 \n",
      "\n",
      "Modelo 2. Loss: 0.03181667998433113, MAE: 0.14311639964580536 \n",
      "\n",
      "Modelo 3. Loss: 0.07811465859413147, MAE: 0.21685998141765594 \n",
      "\n",
      "Modelo 4. Loss: 0.29006633162498474, MAE: 0.42743393778800964 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "losses = [history.history['loss'][-1], history2.history['loss'][-1], history3.history['loss'][-1],history4.history['loss'][-1]]\n",
    "maes = [history.history['mean_absolute_error'][-1], history2.history['mean_absolute_error'][-1], history3.history['mean_absolute_error'][-1],history4.history['mean_absolute_error'][-1]]\n",
    "\n",
    "for i in range(len(losses)):\n",
    "    print(f'Modelo {i+1}. Loss: {losses[i]}, MAE: {maes[i]} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, el modelo con menor MSE (pérdida) y menor MAE es el del primer experimento con una simple hidden layer. Significando que las predicciones del modelo están, en promedio, bastante cerca de los valores reales. Mientras que el modelo con mayor MSE y MAE es el experimento 4 con dropout y batch normalization, seguido por el experimento 3 con dropout.\n",
    "Esto sugiere que el problema de predicción de calificaciones no requiere una arquitectura compleja y podría estar beneficiándose de la simplicidad, evitando así el riesgo de sobreajuste que puede ocurrir con capas adicionales o técnicas como el dropout y batch normalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
